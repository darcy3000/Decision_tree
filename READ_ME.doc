************************DECISION TREE LEARNING*******************


This assignment contains implementataion of  ID3 algorithm along with reduced error pruning and random forest model.

Steps for building a decision tree classifier

Step1:
	Preprocessing and Cleaning the Data.
	The data contained missing values for some attributes represented by "?".
	For Discrete values the "?" was replaced by mode of that attribute.
	For Continuous values the "?" was replaced by mean of that attribute.
	Attributes taking continuous values were split into ranges mapping to boolean values. 

Step2:
	Building the "overfit" tree
	A node was created.
	Base cases are:
		If all instances in a particular space are ">50K" we assigned the target value as positive.
		If all instances in a particular space are "<=50K" we assigned the target value as negative.
		If the space is we assigned the most common target value.

	Inductive Case:
		Find information gain for all attributes and select that with the highest value.
		Assign that attribute to the node.
		Start branching the node on the codomain of attribute value.
		For each member in the codomain:
			Step2
	ACCURACY:83%

Step3:
	Prunning The tree:
	Reduced error prunning was implemented.
	First split the training  data into data for building the tree and validation.
	Starting with the leaves for each node remove its subtree and assign the most common 
	target value.
	Now check for accuracy on the validation data.
	IF the accuracy decreases then rollback to the previous consistent tree.
	ACCURACY:83.4%

Step4:
	Random Forest:
	Divide the data in n(in this case 10) disjoint parts.
	Build a Decision tree for each disjoint part.
	While Classifying:
		Feed the instance to each decision tree.
		Output the target value as predicted by majority of decision trees.
	ACCURACY: 82%

TEAM MEMBERS:
	2015A7PS0080H-NAVNEET KUMAR
	2015A7PS0107H-RAJAT BISWAS
	2015A7PS0111H-ABHIJEET BAJAJ
